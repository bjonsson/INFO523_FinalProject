# Following along to this tutorial by Cristi Vlad:
# https://www.youtube.com/watch?v=1FbBaIU8u5A&list=PLonlF40eS6nzc7TqDshRo7k-mTM1Tu_j4&index=3

# This program detects malware based on the file's PE headers. PE files can only be safely examined on an
# isolated machine, like a virtual machine.

import pandas as pd

# The first 41,323 files/rows are legitimate, whereas the next 96,724 files/rows are from virusshare.com.
malData = pd.read_csv("MalwareData.csv", sep= "|")

# We're dropping the "legitimate" column in favor of separating the data into malware and legitimate files.
legit = malData[0:41323].drop(["legitimate"], axis=1)  # Axis 1 refers to columns. 0 refers to rows.
mal = malData[41323::].drop(["legitimate"], axis=1)

print("The shape of the legit dataset is: %s samples, %s features"%(legit.shape[0],legit.shape[1]))
print("The shape of the malware dataset is: %s samples, %s features"%(mal.shape[0], mal.shape[1]))

# These are all the variables before dropping the "legitimate" column
print(malData.columns)

# Our first machine learning classifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
                                    # Cross validation is mentioned as one of the newer improvements on
                                    # data mining techniques for cybersecurity on p. 49 of
                                    # "Data Mining and Machine Learning for Cybersecurity" by Du and Dua.

# ExtraTrees is a decorrelated, ensemble machine learning decision tree
data_in = malData.drop(['Name', 'md5', 'legitimate'], axis=1).values
labels = malData['legitimate'].values
extratrees = ExtraTreesClassifier().fit(data_in, labels)
select = SelectFromModel(extratrees, prefit=True)
data_in_new = select.transform(data_in)
print(data_in.shape, data_in_new.shape)

# Features are independent variables
import numpy as np
features = data_in_new.shape[1] # features = variables = columns
importances = extratrees.feature_importances_
indices = np.argsort(importances)[::-1] # Sorts the variables in terms of importance

# Prints the variables in descending order of importance. +2 because we eventually remove the first two columns
for f in range(features):
    print("%d"%(f+1), malData.columns[2+indices[f]],importances[indices[f]])

# Training the RandomForestClassifier, which boosts the generalizability of the machine
from sklearn.ensemble import RandomForestClassifier
legit_train, legit_test, mal_train, mal_test = train_test_split(data_in_new, labels, test_size=0.2)
classif = RandomForestClassifier(n_estimators=50)

# Fit standardizes/prepares data then fits to a transformer like a .score()
classif.fit(legit_train, mal_train)

print("The percentage of accuracy of the model: ", classif.score(legit_test, mal_test)*100)

# Model is finished.




# Testing for false positives/negatives
from sklearn.metrics import confusion_matrix

# Running a confusion matrix. A confusion matrix compares actual values with predicted values.
# .predict() predicts the class for new data instances using the classification model
result = classif.predict(legit_test)
conf_mat = confusion_matrix(mal_test,result)

# We are expecting a 2x2 matrix (standard size for Python confusion matrix function)
print(conf_mat.shape)
# Confirming that this is a multidimensional array
print(type(conf_mat))

# We can see that the malware errors were on top, and the legit data's errors were on the bottom
print(conf_mat)
# We selected values that were "True" for each data set, so ones that were not malware are on top,
# and ones there were malware and got through are on the bottom.
print("Percentage of false positives: ",conf_mat[0][1]/sum(conf_mat[0]*100))
print("Percentage of false negatives: ",conf_mat[1][0]/sum(conf_mat[1]*100))
